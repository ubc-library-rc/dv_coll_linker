{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dv_coll_linker \u00b6 Automated study linking for Dataverse installations \u00b6 Introduction \u00b6 Some time in the history of Dataverse 5.x, probably around version 5.6, collection level links in a Dataverse installation ceased to function correctly. More specifically, if collection level links were present,the collection title was linked, but the contents did not appear. For example, given Collections A and B, and Collection B is linked to Collection A: Expected behaviour: Collection A Collection A studies Collection B linked collection entry Collection B studies That is, the studies from Collection B will appear while looking at Collection A. At the very least, this was the behaviour with v4.20. Actual behaviour: Collection A Collection A studies Collection B linked collection entry Further details and links to Github/issues are available here . Quick install, or I hate reading \u00b6 pip install git+https://github.com/ubc-library-rc/dv_coll_linker@master The collection linker \u00b6 This software rectifies this issue externally by creating individual links to studies where formerly there was only a collection link. The software exists as both a Python 3 module and a command line application. Most users will just need the command line application, but the module is present in case the application does not meet user needs. The collection linker: tracks changes over time links newly added items links applicable items when collections are newly linked unlinks items when collections are unlinked It\u2019s designed to be a drop-in replacement returning the former functionality without touching the existing Dataverse code base. It is a standalone product and does not (necessarily) have to be run from the server which hosts a Dataverse installation.","title":"Overview"},{"location":"#dv_coll_linker","text":"","title":"dv_coll_linker"},{"location":"#automated-study-linking-for-dataverse-installations","text":"","title":"Automated study linking for Dataverse installations"},{"location":"#introduction","text":"Some time in the history of Dataverse 5.x, probably around version 5.6, collection level links in a Dataverse installation ceased to function correctly. More specifically, if collection level links were present,the collection title was linked, but the contents did not appear. For example, given Collections A and B, and Collection B is linked to Collection A: Expected behaviour: Collection A Collection A studies Collection B linked collection entry Collection B studies That is, the studies from Collection B will appear while looking at Collection A. At the very least, this was the behaviour with v4.20. Actual behaviour: Collection A Collection A studies Collection B linked collection entry Further details and links to Github/issues are available here .","title":"Introduction"},{"location":"#quick-install-or-i-hate-reading","text":"pip install git+https://github.com/ubc-library-rc/dv_coll_linker@master","title":"Quick install, or I hate reading"},{"location":"#the-collection-linker","text":"This software rectifies this issue externally by creating individual links to studies where formerly there was only a collection link. The software exists as both a Python 3 module and a command line application. Most users will just need the command line application, but the module is present in case the application does not meet user needs. The collection linker: tracks changes over time links newly added items links applicable items when collections are newly linked unlinks items when collections are unlinked It\u2019s designed to be a drop-in replacement returning the former functionality without touching the existing Dataverse code base. It is a standalone product and does not (necessarily) have to be run from the server which hosts a Dataverse installation.","title":"The collection linker"},{"location":"api_reference/","text":"Complete API Reference \u00b6 dv_coll_linker \u00b6 Dataverse collection linker for Dataverse 5.6+ to emulate the functionality of the collection linking in which functioned correctly in Dataverse 4.20 In essence, this software can create individual item links where collection links were. That is, for a collection containing 10 files, this product will create individual links to the 10 files instead of just using the (non-functioning) link to the collection. search: institutes date level searching via Dataverse\u2019s search API linker: Python implementation of linking and unlinking data sets monitor: SQLite monitor of collection level linking. app: Implementation of a standalone application which can run at intervals to emulate the collection linking feature. If run on the server hosting a Dataverse instance, it can read the collection level linking from the datverselinkingdataverse table if supplied the proper credentials for the PostreSQL database underlying Dataverse. If connections fail or the product is run on a platform with no access to the PostgreSQL database, the product will still function but the underlying SQLite database holding dataverse and link information will need to be populated manually. That is, the tables collections and children will need to be manually populated. Similarly, if changes are made to the collection structure, such as an unlinked collection, if the PostgreSQL database cannot be read then the changes must be added to the SQLite database manually. dv_coll_linker.monitor \u00b6 Database creation for study level links in a Dataverse installation. Required because collection level links were apparently broken in v5.? + This module creates the database and, if running on the server with a Dataverse installation, can automatically add any detected linked collections for use with the linker and search modules. This module basically acts as a singleton object on import. init \u00b6 init(dbname: str) -> sqlite3.Connection Intialize database with {dbname}. get_pg_data \u00b6 get_pg_data(dbname: str, user: str, password: str, host: str = 'localhost', port: int = 5432) -> (list, list) Grabs linking data from postgres database. Returns two lists used to populate the collections and children tables populate_db \u00b6 populate_db(conn: sqlite3.Connection, collections: list, children: list) -> bool Updates the database with linking dataverse info fetch_parent_child_collections \u00b6 fetch_parent_child_collections(conn: sqlite3.Connection) -> list Returns a list of (parent, child) collection pairs add_single_study \u00b6 add_single_study(conn: sqlite3.Connection, **kwargs) -> None Writes metadata from a single study to the database. Supply the JSON from the record as keyword arguments. purge_nonexistent \u00b6 purge_nonexistent(conn: sqlite3.Connection, allrecs: dict) -> None Removes PIDS that don\u2019t appear in allrecs[\u2018data\u2019][\u2018items\u2019] check_link_old \u00b6 check_link_old(conn: sqlite3.Connection, pid: str) -> bool Check for existence of link. Returns True if link exists, else False: check_link \u00b6 check_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> bool Check for existence of link. Returns True if link exists, else False: add_link \u00b6 add_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> None desc check_unlink \u00b6 check_unlink(conn: sqlite3.Connection) -> tuple Returns persistent IDs where a link exists but a parent/child relationship does not. That is, it detects the items in a collection that has been unlinked from a parent remove_link \u00b6 remove_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> None desc write_status \u00b6 write_status(conn: sqlite3.Connection, last_check: str, last_count: int) -> None Writes timestamp and total file count to database get_last_count \u00b6 get_last_count(conn: sqlite3.Connection) -> (str, int) Returns the last count of studies in a Dataverse installation get_search_data \u00b6 get_search_data(conn: sqlite3.Connection) -> dict Retrieves last harvested search results write_search_data \u00b6 write_search_data(conn: sqlite3.Connection, last_check: str, search_json: dict) -> None Writes the current study search JSON to the database dv_coll_linker.linker \u00b6 Implementation of Dataverse study linking as Python functions. Nothing fancy, but it does add log messages on failures. create_link \u00b6 create_link(pid: str, parent: str, url: str, key: str, timeout: int = 100) -> bool Create a dataverse link of pid to collection parent. Returns true on successful (new) link. pid: str Dataverse persistent ID (handle or DOI) parent: str Parent (target) collection short name url: str Base url to Dataverse installation key: str API key for Dataverse installation. Note: linking requires superuser privileges. timeout: int Timeout in seconds unlink \u00b6 unlink(pid: str, parent: str, url: str, key: str, timeout: int = 100) -> bool Removes a Dataverse link of pid to collection parent. Returns true on successful removal. pid: str Dataverse persistent ID (handle or DOI) parent: str Parent (target) collection short name url: str Base url to Dataverse installation key: str API key for Dataverse installation. Note: linking requires superuser privileges. timeout: int Timeout in seconds dv_coll_linker.app \u00b6 Collection level linker for Dataverse data repository software. As the current (June 2022) version of Dataverse v5.10 has a bug which has apparently disabled collection level linking. For more information on this bug, please see the Dataverse User\u2019s Community Google group at: https://groups.google.com/g/dataverse-community/c/DDmVelt3Zfk : This part of the dv_coll_linker package will automate per-study linking for collections which are listed as being linked, restoring the functionality found in Dataverse v4+. This involves no changes to the underlying code. Links are tracked in a user specified SQLite database, and linking is performed via the Dataverse API. There are several caveats to this software: Link tables can only be populated automatically if the software has access to the Dataverse PostgreSQL database which usually means that the software will run on the server which hosts the database. This is not techincally required, though, and if the collection and children tables are populated manually, either by importing a PostgreSQL dump or by entering them labouriously by hand, the software will function. This means, however, that new collection links will not be discovered automatically and will need to be added by hand or by reimporting the tables. The software requires superuser Dataverse API keys, as linking is a superuser function. It\u2019s theoretically possible that a superuser does not have direct server access, so if that is the case, please re-read number 1 and speak to the administrator of the Datavese server to obtain the initial data with which to populate your salite database. The software can be run repeatedly to include new links or remove old ones (although old links should automatically be removed if a study is deleted anyway, as it should be purged from the PostgreSQL database. The easiest way to do this is to schedule it to run at regular intervals, such as by using crontab or the windows scheduler. There are other ways to daemonize, but this is supposed to be a stopgap and not a permanent solution, so they have not been implemented. Unlinking code is commented out in the code below. See also linker.unlink. argument_parser \u00b6 argument_parser() -> argparse.ArgumentParser Argument parser for command line script. console_logger \u00b6 console_logger(level: int) -> logging.getLogger Logging to console only. rotating_logger \u00b6 rotating_logger(path: str, level: int, fname='dv_coll_linker.log') -> logging.getLogger Rotating log called called logname, where logname is the full path (ie, /path/to/ main \u00b6 main() Primary testme \u00b6 testme() This is just a test to see if the hook worked dv_coll_linker.search \u00b6 A theoretical date search for Dataverse installations. Inelegant. get_total_records \u00b6 get_total_records(baseurl: str = 'https://abacus.library.ubc.ca') -> int Returns the total number of datasets in the root collection of a Dataverse installation (ie, total number of data sets) get_all_recs \u00b6 get_all_recs(baseurl: str = 'https://abacus.library.ubc.ca', per_page: int = 100, timeout: int = 100) -> dict Returns a single, non-paginated json from the Dataverse search API including all datasets (only). baseurl : str Base url of Dataverse installation per_page : int Number of results per page timeout : int Request timeout in second get_new_recs \u00b6 get_new_recs(allrecs: dict, last_check=str) -> list Returns a list of records (ie, individual items from allrecs[\u2018data\u2019][\u2018items\u2019] that are newer than last_check. Note that \u201cnewer\u201d in this case is to the second, so searching for \u201c2022\u201d will pull up all records for 2022, as 2022 will be automatically encoded to 2022-01-01T00:00:00Z. last_check is a time string in \u2018%Y-%m-%dT%H:%M:%SZ\u2019, or portions thereof. When including the time, make sure to use/include T. dv_coll_linker.data \u00b6","title":"API Reference"},{"location":"api_reference/#complete-api-reference","text":"","title":"Complete API Reference"},{"location":"api_reference/#dv_coll_linker","text":"Dataverse collection linker for Dataverse 5.6+ to emulate the functionality of the collection linking in which functioned correctly in Dataverse 4.20 In essence, this software can create individual item links where collection links were. That is, for a collection containing 10 files, this product will create individual links to the 10 files instead of just using the (non-functioning) link to the collection. search: institutes date level searching via Dataverse\u2019s search API linker: Python implementation of linking and unlinking data sets monitor: SQLite monitor of collection level linking. app: Implementation of a standalone application which can run at intervals to emulate the collection linking feature. If run on the server hosting a Dataverse instance, it can read the collection level linking from the datverselinkingdataverse table if supplied the proper credentials for the PostreSQL database underlying Dataverse. If connections fail or the product is run on a platform with no access to the PostgreSQL database, the product will still function but the underlying SQLite database holding dataverse and link information will need to be populated manually. That is, the tables collections and children will need to be manually populated. Similarly, if changes are made to the collection structure, such as an unlinked collection, if the PostgreSQL database cannot be read then the changes must be added to the SQLite database manually.","title":"dv_coll_linker"},{"location":"api_reference/#dv_coll_linkermonitor","text":"Database creation for study level links in a Dataverse installation. Required because collection level links were apparently broken in v5.? + This module creates the database and, if running on the server with a Dataverse installation, can automatically add any detected linked collections for use with the linker and search modules. This module basically acts as a singleton object on import.","title":"dv_coll_linker.monitor"},{"location":"api_reference/#init","text":"init(dbname: str) -> sqlite3.Connection Intialize database with {dbname}.","title":"init"},{"location":"api_reference/#get_pg_data","text":"get_pg_data(dbname: str, user: str, password: str, host: str = 'localhost', port: int = 5432) -> (list, list) Grabs linking data from postgres database. Returns two lists used to populate the collections and children tables","title":"get_pg_data"},{"location":"api_reference/#populate_db","text":"populate_db(conn: sqlite3.Connection, collections: list, children: list) -> bool Updates the database with linking dataverse info","title":"populate_db"},{"location":"api_reference/#fetch_parent_child_collections","text":"fetch_parent_child_collections(conn: sqlite3.Connection) -> list Returns a list of (parent, child) collection pairs","title":"fetch_parent_child_collections"},{"location":"api_reference/#add_single_study","text":"add_single_study(conn: sqlite3.Connection, **kwargs) -> None Writes metadata from a single study to the database. Supply the JSON from the record as keyword arguments.","title":"add_single_study"},{"location":"api_reference/#purge_nonexistent","text":"purge_nonexistent(conn: sqlite3.Connection, allrecs: dict) -> None Removes PIDS that don\u2019t appear in allrecs[\u2018data\u2019][\u2018items\u2019]","title":"purge_nonexistent"},{"location":"api_reference/#check_link_old","text":"check_link_old(conn: sqlite3.Connection, pid: str) -> bool Check for existence of link. Returns True if link exists, else False:","title":"check_link_old"},{"location":"api_reference/#check_link","text":"check_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> bool Check for existence of link. Returns True if link exists, else False:","title":"check_link"},{"location":"api_reference/#add_link","text":"add_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> None desc","title":"add_link"},{"location":"api_reference/#check_unlink","text":"check_unlink(conn: sqlite3.Connection) -> tuple Returns persistent IDs where a link exists but a parent/child relationship does not. That is, it detects the items in a collection that has been unlinked from a parent","title":"check_unlink"},{"location":"api_reference/#remove_link","text":"remove_link(conn: sqlite3.Connection, pid: str, parent: str, child: str) -> None desc","title":"remove_link"},{"location":"api_reference/#write_status","text":"write_status(conn: sqlite3.Connection, last_check: str, last_count: int) -> None Writes timestamp and total file count to database","title":"write_status"},{"location":"api_reference/#get_last_count","text":"get_last_count(conn: sqlite3.Connection) -> (str, int) Returns the last count of studies in a Dataverse installation","title":"get_last_count"},{"location":"api_reference/#get_search_data","text":"get_search_data(conn: sqlite3.Connection) -> dict Retrieves last harvested search results","title":"get_search_data"},{"location":"api_reference/#write_search_data","text":"write_search_data(conn: sqlite3.Connection, last_check: str, search_json: dict) -> None Writes the current study search JSON to the database","title":"write_search_data"},{"location":"api_reference/#dv_coll_linkerlinker","text":"Implementation of Dataverse study linking as Python functions. Nothing fancy, but it does add log messages on failures.","title":"dv_coll_linker.linker"},{"location":"api_reference/#create_link","text":"create_link(pid: str, parent: str, url: str, key: str, timeout: int = 100) -> bool Create a dataverse link of pid to collection parent. Returns true on successful (new) link. pid: str Dataverse persistent ID (handle or DOI) parent: str Parent (target) collection short name url: str Base url to Dataverse installation key: str API key for Dataverse installation. Note: linking requires superuser privileges. timeout: int Timeout in seconds","title":"create_link"},{"location":"api_reference/#unlink","text":"unlink(pid: str, parent: str, url: str, key: str, timeout: int = 100) -> bool Removes a Dataverse link of pid to collection parent. Returns true on successful removal. pid: str Dataverse persistent ID (handle or DOI) parent: str Parent (target) collection short name url: str Base url to Dataverse installation key: str API key for Dataverse installation. Note: linking requires superuser privileges. timeout: int Timeout in seconds","title":"unlink"},{"location":"api_reference/#dv_coll_linkerapp","text":"Collection level linker for Dataverse data repository software. As the current (June 2022) version of Dataverse v5.10 has a bug which has apparently disabled collection level linking. For more information on this bug, please see the Dataverse User\u2019s Community Google group at: https://groups.google.com/g/dataverse-community/c/DDmVelt3Zfk : This part of the dv_coll_linker package will automate per-study linking for collections which are listed as being linked, restoring the functionality found in Dataverse v4+. This involves no changes to the underlying code. Links are tracked in a user specified SQLite database, and linking is performed via the Dataverse API. There are several caveats to this software: Link tables can only be populated automatically if the software has access to the Dataverse PostgreSQL database which usually means that the software will run on the server which hosts the database. This is not techincally required, though, and if the collection and children tables are populated manually, either by importing a PostgreSQL dump or by entering them labouriously by hand, the software will function. This means, however, that new collection links will not be discovered automatically and will need to be added by hand or by reimporting the tables. The software requires superuser Dataverse API keys, as linking is a superuser function. It\u2019s theoretically possible that a superuser does not have direct server access, so if that is the case, please re-read number 1 and speak to the administrator of the Datavese server to obtain the initial data with which to populate your salite database. The software can be run repeatedly to include new links or remove old ones (although old links should automatically be removed if a study is deleted anyway, as it should be purged from the PostgreSQL database. The easiest way to do this is to schedule it to run at regular intervals, such as by using crontab or the windows scheduler. There are other ways to daemonize, but this is supposed to be a stopgap and not a permanent solution, so they have not been implemented. Unlinking code is commented out in the code below. See also linker.unlink.","title":"dv_coll_linker.app"},{"location":"api_reference/#argument_parser","text":"argument_parser() -> argparse.ArgumentParser Argument parser for command line script.","title":"argument_parser"},{"location":"api_reference/#console_logger","text":"console_logger(level: int) -> logging.getLogger Logging to console only.","title":"console_logger"},{"location":"api_reference/#rotating_logger","text":"rotating_logger(path: str, level: int, fname='dv_coll_linker.log') -> logging.getLogger Rotating log called called logname, where logname is the full path (ie, /path/to/","title":"rotating_logger"},{"location":"api_reference/#main","text":"main() Primary","title":"main"},{"location":"api_reference/#testme","text":"testme() This is just a test to see if the hook worked","title":"testme"},{"location":"api_reference/#dv_coll_linkersearch","text":"A theoretical date search for Dataverse installations. Inelegant.","title":"dv_coll_linker.search"},{"location":"api_reference/#get_total_records","text":"get_total_records(baseurl: str = 'https://abacus.library.ubc.ca') -> int Returns the total number of datasets in the root collection of a Dataverse installation (ie, total number of data sets)","title":"get_total_records"},{"location":"api_reference/#get_all_recs","text":"get_all_recs(baseurl: str = 'https://abacus.library.ubc.ca', per_page: int = 100, timeout: int = 100) -> dict Returns a single, non-paginated json from the Dataverse search API including all datasets (only). baseurl : str Base url of Dataverse installation per_page : int Number of results per page timeout : int Request timeout in second","title":"get_all_recs"},{"location":"api_reference/#get_new_recs","text":"get_new_recs(allrecs: dict, last_check=str) -> list Returns a list of records (ie, individual items from allrecs[\u2018data\u2019][\u2018items\u2019] that are newer than last_check. Note that \u201cnewer\u201d in this case is to the second, so searching for \u201c2022\u201d will pull up all records for 2022, as 2022 will be automatically encoded to 2022-01-01T00:00:00Z. last_check is a time string in \u2018%Y-%m-%dT%H:%M:%SZ\u2019, or portions thereof. When including the time, make sure to use/include T.","title":"get_new_recs"},{"location":"api_reference/#dv_coll_linkerdata","text":"","title":"dv_coll_linker.data"},{"location":"faq/","text":"Frequently asked questions \u00b6 Why don\u2019t you just fix the Dataverse code? Writing a Python utility is much easier than examining 10 years of someone else\u2019s Java and datebase schema and testing and debugging. This method uses an easily reversible method to perform the same thing with significantly less effort For operational reasons, not everyone can update to the most current version of Dataverse immediately. This conveniently sidesteps the issue. How do I reverse the changes made by dv_coll_linker? The links table in the sqlite database shows all the links created by the software. You can feed that table to dv_coll_linker.linker.unlink or use requests or urllib to remove the links. I am not a superuser, nor do I have access to our server. What can I do? You can add your voice to Dataverse\u2019s Github issues page and ask for a fix.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"Why don\u2019t you just fix the Dataverse code? Writing a Python utility is much easier than examining 10 years of someone else\u2019s Java and datebase schema and testing and debugging. This method uses an easily reversible method to perform the same thing with significantly less effort For operational reasons, not everyone can update to the most current version of Dataverse immediately. This conveniently sidesteps the issue. How do I reverse the changes made by dv_coll_linker? The links table in the sqlite database shows all the links created by the software. You can feed that table to dv_coll_linker.linker.unlink or use requests or urllib to remove the links. I am not a superuser, nor do I have access to our server. What can I do? You can add your voice to Dataverse\u2019s Github issues page and ask for a fix.","title":"Frequently asked questions"},{"location":"install/","text":"Installation \u00b6 Prequisites \u00b6 Python >= v.3.6 For automatic database population, Dataverse server access and access to Dataverse\u2019s PostgreSQL database Installation \u00b6 Generally, installation is performed using pip : pip install git+https://github.com/ubc-library-rc/dv_coll_linker@master Alternately, it is possible to install using one of the many other ways to install Python packages dv_coll_linker is not on PyPi ; installing from git or downloading from source is the only option at this time. Post-install \u00b6 After installation, most users will us the command line script, invoked by dv_coll_linker . See the usage page for more details. Using the package within Python is the same as any other. Importing dv_coll_linker on its own does nothing, however. Each module must be imported on its own. For example: import dv_coll_linker.search or from dv_coll_linker import search If using dv_coll_linker as Python package, you may wish to consult the API reference .","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#prequisites","text":"Python >= v.3.6 For automatic database population, Dataverse server access and access to Dataverse\u2019s PostgreSQL database","title":"Prequisites"},{"location":"install/#installation_1","text":"Generally, installation is performed using pip : pip install git+https://github.com/ubc-library-rc/dv_coll_linker@master Alternately, it is possible to install using one of the many other ways to install Python packages dv_coll_linker is not on PyPi ; installing from git or downloading from source is the only option at this time.","title":"Installation"},{"location":"install/#post-install","text":"After installation, most users will us the command line script, invoked by dv_coll_linker . See the usage page for more details. Using the package within Python is the same as any other. Importing dv_coll_linker on its own does nothing, however. Each module must be imported on its own. For example: import dv_coll_linker.search or from dv_coll_linker import search If using dv_coll_linker as Python package, you may wish to consult the API reference .","title":"Post-install"},{"location":"usage/","text":"Command Line Usage \u00b6 Unless the Dataverse installation has an unusual use case or setup, most users will just need to run the included command line utilty, called, unsurprisingly, dv_coll_linker . Notes on running the software outside the Dataverse server environment While it is technically not required to run the collection linker from the server which hosts a Dataverse installation, it may be desirable to do so. If the user running the program has sufficient privileges, if run on the server the dv_coll_linker utility will automatically detect linked collections and populate the resultant sqlite3 database with the appropriate data, allowing the software to run more-or-less automatically. If the database is not automatically populated, the children table in the tracking database must be populated manually. It\u2019s quite straightforward: parent_id: Database id number of parent parent_alias: Dataverse short name for collection child_id: Database id number of child (ie, the collection which is linked to the parent) child_alias: Dataverse short name for the child collection. For those not comfortable using sqlite3 from the command line, there is a very nice free and open source piece of software with a well-designed UI: DB Browser for SQLite . It may be possible to have the utility automatically populate the tables if you have the PostgreSQL port open to outside, but that seems unlikely and inadvisable. Using the command line tool: \u00b6 To use the tool on the server and automatically harvest data from PostgreSQL, you will need: The Dataverse installation\u2019s PostgreSQL database name The password to access the database The port for accessing the database A superuser API key Superuser keys are required because links may only be produced by superusers. Some defaults are already in place as listed below. dv_coll_linker \u00b6 usage: dv_coll_linker [-h] [-u URL] [-d DVDBNAME] [-y USER] [-w PASSWORD] [-p PORT] [-r DBHOST] [-k KEY] [-b DBNAME] [-l LOG] [-v--version] Study level linker for Dataverse installations. Finds collection level links and automatically adds study-level links for each found collection. Addresses issues as noted here https://groups.google.com/g/dataverse-community/c/DDmVelt3Zfk optional arguments: -h, --help show this help message and exit -u URL, --url URL URL to base Dataverse installation. Default https://abacus.library.ubc.ca -d DVDBNAME, --dvdbname DVDBNAME Dataverse PostgreSQL Database name. Defaults to dvndb. If not run on the server hosting a Dataverse installation, the program will attempt to connect to PostgreSQL. It will likely fail if your security is good and you will need to populate the collection and children database manually. -y USER, --user USER PostgreSQL database username. Defaults to dvnapp -w PASSWORD, --password PASSWORD PostgreSQL database password for --user.Defaults to 123456. No, of course it doesn't. There is no default password. -p PORT, --port PORT PostgreSQL port. Defaults to 5432 -r DBHOST, --dbhost DBHOST Database host. Defaults to localhost. If using another host use *just* the hostname. -k KEY, --key KEY Dataverse installation API key. This *must* be a superuser API key. Non-superuser keys will fail. If you don't know if you have superuser keys, you are probably not a superuser. -b DBNAME, --dbname DBNAME Local sqlite3 link database. Defaults to ~/dv_links.sqlite3 -l LOG, --log LOG Log directory. Defaults to ~/logs. Log is saved as dv_coll_linker.log -v--version Show version number and exit Continual updating \u00b6 There are a few options for ensuring automatic updates to your dataverse installation. Run at intervals with cron This is probably advisable with large or multi-user Dataverse installations. As the utility only obtains new or changed studies, it does not place excessive server load and can be run at fairly frequent intervals, such as 10 minutes. Note that the first time the software runs, the whole Dataverse installation will be crawled for metadata, so if server load is an issue, you should be aware of this Manually running For smaller installations or those with few linked collections, it may be easier to just run the utility on an ad_hoc basis. Windows scheduler If, for some reason, you are not running the software on the server and are using a Windows computer, you can use the Windows Task Scheduler to run the application at your desired interval.","title":"Command Line Usage"},{"location":"usage/#command-line-usage","text":"Unless the Dataverse installation has an unusual use case or setup, most users will just need to run the included command line utilty, called, unsurprisingly, dv_coll_linker . Notes on running the software outside the Dataverse server environment While it is technically not required to run the collection linker from the server which hosts a Dataverse installation, it may be desirable to do so. If the user running the program has sufficient privileges, if run on the server the dv_coll_linker utility will automatically detect linked collections and populate the resultant sqlite3 database with the appropriate data, allowing the software to run more-or-less automatically. If the database is not automatically populated, the children table in the tracking database must be populated manually. It\u2019s quite straightforward: parent_id: Database id number of parent parent_alias: Dataverse short name for collection child_id: Database id number of child (ie, the collection which is linked to the parent) child_alias: Dataverse short name for the child collection. For those not comfortable using sqlite3 from the command line, there is a very nice free and open source piece of software with a well-designed UI: DB Browser for SQLite . It may be possible to have the utility automatically populate the tables if you have the PostgreSQL port open to outside, but that seems unlikely and inadvisable.","title":"Command Line Usage"},{"location":"usage/#using-the-command-line-tool","text":"To use the tool on the server and automatically harvest data from PostgreSQL, you will need: The Dataverse installation\u2019s PostgreSQL database name The password to access the database The port for accessing the database A superuser API key Superuser keys are required because links may only be produced by superusers. Some defaults are already in place as listed below.","title":"Using the command line tool:"},{"location":"usage/#dv_coll_linker","text":"usage: dv_coll_linker [-h] [-u URL] [-d DVDBNAME] [-y USER] [-w PASSWORD] [-p PORT] [-r DBHOST] [-k KEY] [-b DBNAME] [-l LOG] [-v--version] Study level linker for Dataverse installations. Finds collection level links and automatically adds study-level links for each found collection. Addresses issues as noted here https://groups.google.com/g/dataverse-community/c/DDmVelt3Zfk optional arguments: -h, --help show this help message and exit -u URL, --url URL URL to base Dataverse installation. Default https://abacus.library.ubc.ca -d DVDBNAME, --dvdbname DVDBNAME Dataverse PostgreSQL Database name. Defaults to dvndb. If not run on the server hosting a Dataverse installation, the program will attempt to connect to PostgreSQL. It will likely fail if your security is good and you will need to populate the collection and children database manually. -y USER, --user USER PostgreSQL database username. Defaults to dvnapp -w PASSWORD, --password PASSWORD PostgreSQL database password for --user.Defaults to 123456. No, of course it doesn't. There is no default password. -p PORT, --port PORT PostgreSQL port. Defaults to 5432 -r DBHOST, --dbhost DBHOST Database host. Defaults to localhost. If using another host use *just* the hostname. -k KEY, --key KEY Dataverse installation API key. This *must* be a superuser API key. Non-superuser keys will fail. If you don't know if you have superuser keys, you are probably not a superuser. -b DBNAME, --dbname DBNAME Local sqlite3 link database. Defaults to ~/dv_links.sqlite3 -l LOG, --log LOG Log directory. Defaults to ~/logs. Log is saved as dv_coll_linker.log -v--version Show version number and exit","title":"dv_coll_linker"},{"location":"usage/#continual-updating","text":"There are a few options for ensuring automatic updates to your dataverse installation. Run at intervals with cron This is probably advisable with large or multi-user Dataverse installations. As the utility only obtains new or changed studies, it does not place excessive server load and can be run at fairly frequent intervals, such as 10 minutes. Note that the first time the software runs, the whole Dataverse installation will be crawled for metadata, so if server load is an issue, you should be aware of this Manually running For smaller installations or those with few linked collections, it may be easier to just run the utility on an ad_hoc basis. Windows scheduler If, for some reason, you are not running the software on the server and are using a Windows computer, you can use the Windows Task Scheduler to run the application at your desired interval.","title":"Continual updating"}]}